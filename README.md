# RotoBART

## ToDos:
- Train 128k sentencepiece tokenizer on the Pile
- add sentence permutation to train script
- add text infill to train script
- Ensure fixed sequence lengths (padding), to avoid jax recompilation
- Test on our TPU VM
- Check we're using all TPU capacity
- Add checkpointing
- Model parallel ?
